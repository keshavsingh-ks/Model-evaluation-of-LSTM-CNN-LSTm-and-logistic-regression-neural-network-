{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80381f11",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment Analysis Project\n",
    "\n",
    "## Objective\n",
    "The goal of this project is to build a deep learning model to perform sentiment analysis on a dataset of text. Sentiment analysis is a common task in Natural Language Processing (NLP) where the sentiment (positive, negative, or neutral) of a piece of text is determined. In this project, we will use a deep learning approach to classify the sentiment of tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'train-processed.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cca91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic data exploration\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f79c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Use the tokens column as input data\n",
    "texts = df['tokens'].astype(str).tolist()\n",
    "y = df['sentiment'].tolist()  # Directly use the numeric sentiment labels\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert tokens to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 100  # Adjust this based on the average length of your sequences\n",
    "X = pad_sequences(sequences, padding='post', maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val, train_idx, val_idx = train_test_split(\n",
    "    X, y, df.index, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97909d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {val_acc * 100:.2f}%')\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a077d",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Learning & Hyperparameter Tuning\n",
    "\n",
    "### Deep Learning in Sentiment Analysis\n",
    "In this project, we applied a deep learning model using LSTM (Long Short-Term Memory) layers, which are well-suited for sequence data like text. The model architecture includes embedding layers to convert text into numerical form, followed by LSTM layers to capture temporal dependencies, and dense layers for the final classification.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "Hyperparameter tuning is crucial to optimize the performance of deep learning models. Key hyperparameters in this project include:\n",
    "- **Embedding dimension**: Size of the dense vector for each token.\n",
    "- **LSTM units**: Number of units in the LSTM layers.\n",
    "- **Batch size**: Number of samples per gradient update.\n",
    "- **Learning rate**: Step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "We could further improve the model by using techniques like GridSearchCV or RandomizedSearchCV to find the best combination of these hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7ffed",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this project, we successfully built a deep learning model to perform sentiment analysis on a dataset of tweets. The model achieved a validation accuracy of X%. This project demonstrates the power of deep learning in natural language processing tasks and highlights the importance of hyperparameter tuning in optimizing model performance.\n",
    "\n",
    "This model can be applied to real-world scenarios where sentiment analysis is needed, such as social media monitoring, customer feedback analysis, and more.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
